{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step 0 microbatch loss 285.479980\n",
      "Microbatch test accuracy 8.6%\n",
      "Validation accuracy 26.8%\n",
      "Step 500 microbatch loss 10.859406\n",
      "Microbatch test accuracy 75.8%\n",
      "Validation accuracy 81.5%\n",
      "Test accuracy 87.1%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "layer_size = [image_size * image_size, 1024, 10]\n",
    "num_steps = 3000\n",
    "l2_factor = 0.1 / (image_size * image_size)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_test_labels = tf.constant(test_labels)\n",
    "    \n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(layer_size) - 1):\n",
    "        weights.append(tf.Variable(tf.truncated_normal([layer_size[i], layer_size[i + 1]])))\n",
    "        biases.append(tf.Variable(tf.zeros([layer_size[i + 1]])))\n",
    "    \n",
    "    def forward(x):\n",
    "        layer0 = tf.nn.relu(tf.matmul(x, weights[0]) + biases[0])   \n",
    "        return tf.matmul(layer0, weights[1]) + biases[1]\n",
    "    \n",
    "    logits = forward(tf_train_dataset)\n",
    "    tf_l2_factor = tf.constant(l2_factor)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "            # (tf_l2_factor * tf.nn.l2_loss(weights[0])) + \\          \n",
    "            #(tf_l2_factor * tf.nn.l2_loss(weights[1]))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    train_pred = tf.nn.softmax(logits)\n",
    "    valid_pred = tf.nn.softmax(forward(tf_valid_dataset))\n",
    "    test_pred = tf.nn.softmax(forward(tf_test_dataset))\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        start = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[start : start + batch_size,:]\n",
    "        batch_labels = train_labels[start : start + batch_size]\n",
    "        feed_dict = {tf_train_dataset : batch_data,\n",
    "                    tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_pred], feed_dict = feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Step %d microbatch loss %f\" % (step, l))\n",
    "            print(\"Microbatch test accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy %.1f%%\" % accuracy(valid_pred.eval(), valid_labels))\n",
    "    print(\"Test accuracy %.1f%%\" % accuracy(test_pred.eval(), test_labels))\n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step 0 microbatch loss 308.262512\n",
      "Microbatch test accuracy 7.8%\n",
      "Validation accuracy 28.8%\n",
      "Step 500 microbatch loss 0.000000\n",
      "Microbatch test accuracy 100.0%\n",
      "Validation accuracy 63.2%\n",
      "Test accuracy 68.6%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "layer_size = [image_size * image_size, 1024, 10]\n",
    "num_steps = 1000\n",
    "l2_factor = 0 / (image_size * image_size)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_test_labels = tf.constant(test_labels)\n",
    "    \n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(layer_size) - 1):\n",
    "        weights.append(tf.Variable(tf.truncated_normal([layer_size[i], layer_size[i + 1]])))\n",
    "        biases.append(tf.Variable(tf.zeros([layer_size[i + 1]])))\n",
    "    \n",
    "    def forward(x):\n",
    "        layer0 = tf.nn.relu(tf.matmul(x, weights[0]) + biases[0])   \n",
    "        return tf.matmul(layer0, weights[1]) + biases[1]\n",
    "    \n",
    "    logits = forward(tf_train_dataset)\n",
    "    tf_l2_factor = tf.constant(l2_factor)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "            # (tf_l2_factor * tf.nn.l2_loss(weights[0])) + \\          \n",
    "            #(tf_l2_factor * tf.nn.l2_loss(weights[1]))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    train_pred = tf.nn.softmax(logits)\n",
    "    valid_pred = tf.nn.softmax(forward(tf_valid_dataset))\n",
    "    test_pred = tf.nn.softmax(forward(tf_test_dataset))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        start = (batch_size * 7)\n",
    "        batch_data = train_dataset[start : start + batch_size,:]\n",
    "        batch_labels = train_labels[start : start + batch_size]\n",
    "        feed_dict = {tf_train_dataset : batch_data,\n",
    "                    tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_pred], feed_dict = feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Step %d microbatch loss %f\" % (step, l))\n",
    "            print(\"Microbatch test accuracy %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy %.1f%%\" % accuracy(valid_pred.eval(), valid_labels))\n",
    "    print(\"Test accuracy %.1f%%\" % accuracy(test_pred.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step 0 microbatch loss 832.556580\n",
      "Microbatch test accuracy 10.9%\n",
      "Validation accuracy 18.6%\n",
      "Step 500 microbatch loss 84.218231\n",
      "Microbatch test accuracy 48.4%\n",
      "Validation accuracy 79.9%\n",
      "Step 1000 microbatch loss 70.427795\n",
      "Microbatch test accuracy 42.2%\n",
      "Validation accuracy 76.6%\n",
      "Step 1500 microbatch loss 90.975639\n",
      "Microbatch test accuracy 40.6%\n",
      "Validation accuracy 75.8%\n",
      "Step 2000 microbatch loss 44.950993\n",
      "Microbatch test accuracy 36.7%\n",
      "Validation accuracy 75.3%\n",
      "Step 2500 microbatch loss 40.404484\n",
      "Microbatch test accuracy 39.8%\n",
      "Validation accuracy 74.8%\n",
      "Test accuracy 81.8%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "layer_size = [image_size * image_size, 1024, 10]\n",
    "num_steps = 3000\n",
    "l2_factor = 0.1 / (image_size * image_size)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_test_labels = tf.constant(test_labels)\n",
    "    \n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(layer_size) - 1):\n",
    "        weights.append(tf.Variable(tf.truncated_normal([layer_size[i], layer_size[i + 1]])))\n",
    "        biases.append(tf.Variable(tf.zeros([layer_size[i + 1]])))\n",
    "    \n",
    "    def forward_dropout(x):\n",
    "        layer0 = tf.nn.dropout(tf.nn.relu(tf.matmul(x, weights[0]) + biases[0]),0.5) \n",
    "        return tf.nn.dropout(tf.matmul(layer0, weights[1]) + biases[1], 0.5)\n",
    "    def forward(x):\n",
    "        layer0 = tf.nn.relu(tf.matmul(x, weights[0]) + biases[0])   \n",
    "        return tf.matmul(layer0, weights[1]) + biases[1]\n",
    "    \n",
    "    logits = forward_dropout(tf_train_dataset)\n",
    "    tf_l2_factor = tf.constant(l2_factor)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "    (tf_l2_factor * tf.nn.l2_loss(weights[0])) + \\\n",
    "    (tf_l2_factor * tf.nn.l2_loss(weights[1]))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    train_pred = tf.nn.softmax(forward(tf_train_dataset)\n",
    "    valid_pred = tf.nn.softmax(forward(tf_valid_dataset))\n",
    "    test_pred = tf.nn.softmax(forward(tf_test_dataset))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        start = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[start : start + batch_size,:]\n",
    "        batch_labels = train_labels[start : start + batch_size]\n",
    "        feed_dict = {tf_train_dataset : batch_data,\n",
    "                    tf_train_labels : batch_labels}\n",
    "        _, l, train_pred = session.run([optimizer, loss, train_pred], feed_dict = feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Step %d microbatch loss %f\" % (step, l))\n",
    "            print(\"Microbatch test accuracy %.1f%%\" % accuracy(train_pred.eval(), batch_labels))\n",
    "            print(\"Validation accuracy %.1f%%\" % accuracy(valid_pred.eval(), valid_labels))\n",
    "    print(\"Test accuracy %.1f%%\" % accuracy(test_pred.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built\n"
     ]
    }
   ],
   "source": [
    "encodeHiddenWidth = 500\n",
    "encodeWidth = 100\n",
    "hiddenWidth1 = 1024\n",
    "hiddenWidth2 = 200\n",
    "num_steps = 8000\n",
    "num_steps_Auto = 800\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.35\n",
    "decay_rate = 0.9\n",
    "decay_steps = 2000\n",
    "\n",
    "loss_l2_factor = 0.00 / (image_size * image_size)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Combine tanh-activation autoencoder with 1-hidden-layer ReLU NN\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    #learning_rate = tf.train.exponential_decay(0.5, global_step)\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_test_labels = tf.constant(test_labels)\n",
    "    \n",
    "    encodeHiddenWeight = tf.Variable(tf.truncated_normal([image_size * image_size, encodeHiddenWidth]))\n",
    "    encodeHiddenBias = tf.Variable(tf.zeros([encodeHiddenWidth]))\n",
    "    encodeWeight = tf.Variable(tf.truncated_normal([encodeHiddenWidth, encodeWidth]))\n",
    "    encodeBias = tf.Variable(tf.zeros([encodeWidth]))\n",
    "    decodeWeight = tf.Variable(tf.truncated_normal([encodeWidth, image_size * image_size]))\n",
    "    decodeBias = tf.Variable(tf.zeros([image_size * image_size]))\n",
    "    \n",
    "    def encode(x):\n",
    "        hidden = tf.nn.tanh(tf.matmul(x, encodeHiddenWeight) + encodeHiddenBias)\n",
    "        return tf.nn.tanh(tf.matmul(hidden, encodeWeight) + encodeBias)\n",
    "    \n",
    "    def autoencode(x):\n",
    "        return tf.matmul(encode(x), decodeWeight) + decodeBias\n",
    "    \n",
    "    def magnitude(x):\n",
    "        return tf.reduce_sum(tf.mul(x,x))\n",
    "    lossAuto = magnitude((autoencode(tf_train_dataset) - tf_train_dataset)) / magnitude(tf_train_dataset)\n",
    "    optimizerAuto = tf.train.GradientDescentOptimizer(0.1).minimize(lossAuto)\n",
    "    \n",
    "    weight0 = tf.Variable(tf.truncated_normal([image_size * image_size, hiddenWidth1]))\n",
    "    bias0 = tf.Variable(tf.zeros([hiddenWidth1]))\n",
    "    weight1 = tf.Variable(tf.truncated_normal([hiddenWidth1, hiddenWidth2]))\n",
    "    bias1 = tf.Variable(tf.zeros([hiddenWidth2]))\n",
    "    weight2 = tf.Variable(tf.truncated_normal([hiddenWidth2 + encodeWidth, num_labels]))\n",
    "    bias2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    def forward(x):\n",
    "        layer0 = tf.nn.tanh(tf.matmul(x, weight0) + bias0)\n",
    "        layer1 = tf.nn.tanh(tf.matmul(layer0, weight1) + bias1)\n",
    "        return tf.matmul(tf.concat(1, [layer1, encode(x)]), weight2) + bias2\n",
    "    \n",
    "    def forward_dropout(x):\n",
    "        layer0 = tf.nn.tanh(tf.nn.dropout(tf.matmul(x, weight0) + bias0, 0.5))\n",
    "        layer1 = tf.nn.tanh(tf.nn.dropout(tf.matmul(layer0, weight1) + bias1, 0.5))\n",
    "        return tf.matmul(tf.concat(1, [layer1, encode(x)]), weight2) + bias2\n",
    "    \n",
    "    def my_cross_entropy(logits,labels):\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        return tf.reduce_mean(tf.mul(labels, tf.log(prob + 10 ** (-30))))\n",
    "    \n",
    "    loss = (-1 * my_cross_entropy(forward_dropout(tf_train_dataset), tf_train_labels))\n",
    "    # loss_l2_factor * (tf.nn.l2_loss(weight0) + tf.nn.l2_loss(weight1) + tf.nn.l2_loss(weight2))\n",
    "    rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    train_pred = tf.nn.softmax(forward(tf_train_dataset))\n",
    "    valid_pred = tf.nn.softmax(forward(tf_valid_dataset))\n",
    "    test_pred = tf.nn.softmax(forward(tf_test_dataset))\n",
    "print(\"Graph built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Starting autoencoder training\n",
      "Step 0, Loss 357.678741\n",
      "Step 100, Loss 28.598131\n",
      "Step 200, Loss 32.562088\n",
      "Step 300, Loss 17.582573\n",
      "Step 400, Loss 22.706758\n",
      "Step 500, Loss 13.879094\n",
      "Step 600, Loss 6.125193\n",
      "Step 700, Loss 13.355504\n",
      "Finished autoencoder training\n",
      "Beginning neural net training\n",
      "0: Train accuracy 13.3%, loss: 1.695542\n",
      "Validation accuracy 12.4%\n",
      "100: Train accuracy 21.9%, loss: 0.965698\n",
      "Validation accuracy 23.6%\n",
      "200: Train accuracy 33.6%, loss: 1.095282\n",
      "Validation accuracy 35.4%\n",
      "300: Train accuracy 43.8%, loss: 0.807944\n",
      "Validation accuracy 45.6%\n",
      "400: Train accuracy 57.8%, loss: 0.770340\n",
      "Validation accuracy 52.9%\n",
      "500: Train accuracy 54.7%, loss: 0.711360\n",
      "Validation accuracy 57.8%\n",
      "600: Train accuracy 56.2%, loss: 0.657331\n",
      "Validation accuracy 60.8%\n",
      "700: Train accuracy 71.9%, loss: 0.578146\n",
      "Validation accuracy 63.7%\n",
      "800: Train accuracy 68.8%, loss: 0.519240\n",
      "Validation accuracy 65.2%\n",
      "900: Train accuracy 62.5%, loss: 0.552476\n",
      "Validation accuracy 67.4%\n",
      "1000: Train accuracy 68.8%, loss: 0.442083\n",
      "Validation accuracy 68.7%\n",
      "1100: Train accuracy 68.8%, loss: 0.532775\n",
      "Validation accuracy 69.6%\n",
      "1200: Train accuracy 68.8%, loss: 0.481611\n",
      "Validation accuracy 70.8%\n",
      "1300: Train accuracy 72.7%, loss: 0.415910\n",
      "Validation accuracy 71.3%\n",
      "1400: Train accuracy 75.8%, loss: 0.453889\n",
      "Validation accuracy 72.3%\n",
      "1500: Train accuracy 71.1%, loss: 0.482696\n",
      "Validation accuracy 72.7%\n",
      "1600: Train accuracy 71.1%, loss: 0.400520\n",
      "Validation accuracy 73.2%\n",
      "1700: Train accuracy 69.5%, loss: 0.405993\n",
      "Validation accuracy 73.8%\n",
      "1800: Train accuracy 75.0%, loss: 0.286388\n",
      "Validation accuracy 74.2%\n",
      "1900: Train accuracy 75.8%, loss: 0.394614\n",
      "Validation accuracy 74.2%\n",
      "2000: Train accuracy 73.4%, loss: 0.362173\n",
      "Validation accuracy 74.4%\n",
      "2100: Train accuracy 78.9%, loss: 0.371816\n",
      "Validation accuracy 75.1%\n",
      "2200: Train accuracy 68.0%, loss: 0.339434\n",
      "Validation accuracy 74.9%\n",
      "2300: Train accuracy 72.7%, loss: 0.377130\n",
      "Validation accuracy 75.2%\n",
      "2400: Train accuracy 71.9%, loss: 0.415767\n",
      "Validation accuracy 75.4%\n",
      "2500: Train accuracy 80.5%, loss: 0.298979\n",
      "Validation accuracy 75.9%\n",
      "2600: Train accuracy 77.3%, loss: 0.347460\n",
      "Validation accuracy 75.8%\n",
      "2700: Train accuracy 77.3%, loss: 0.297482\n",
      "Validation accuracy 76.0%\n",
      "2800: Train accuracy 68.8%, loss: 0.379621\n",
      "Validation accuracy 76.1%\n",
      "2900: Train accuracy 81.2%, loss: 0.189152\n",
      "Validation accuracy 76.3%\n",
      "3000: Train accuracy 75.8%, loss: 0.258043\n",
      "Validation accuracy 76.7%\n",
      "3100: Train accuracy 81.2%, loss: 0.258977\n",
      "Validation accuracy 76.7%\n",
      "3200: Train accuracy 75.0%, loss: 0.244004\n",
      "Validation accuracy 76.8%\n",
      "3300: Train accuracy 73.4%, loss: 0.312401\n",
      "Validation accuracy 76.7%\n",
      "3400: Train accuracy 78.1%, loss: 0.181629\n",
      "Validation accuracy 76.7%\n",
      "3500: Train accuracy 70.3%, loss: 0.348642\n",
      "Validation accuracy 77.0%\n",
      "3600: Train accuracy 78.1%, loss: 0.216929\n",
      "Validation accuracy 76.9%\n",
      "3700: Train accuracy 69.5%, loss: 0.251696\n",
      "Validation accuracy 77.1%\n",
      "3800: Train accuracy 69.5%, loss: 0.325845\n",
      "Validation accuracy 77.3%\n",
      "3900: Train accuracy 81.2%, loss: 0.175318\n",
      "Validation accuracy 77.1%\n",
      "4000: Train accuracy 79.7%, loss: 0.240217\n",
      "Validation accuracy 77.1%\n",
      "4100: Train accuracy 75.0%, loss: 0.232496\n",
      "Validation accuracy 77.1%\n",
      "4200: Train accuracy 78.1%, loss: 0.207191\n",
      "Validation accuracy 77.2%\n",
      "4300: Train accuracy 79.7%, loss: 0.248704\n",
      "Validation accuracy 77.3%\n",
      "4400: Train accuracy 72.7%, loss: 0.230760\n",
      "Validation accuracy 77.5%\n",
      "4500: Train accuracy 80.5%, loss: 0.229142\n",
      "Validation accuracy 77.4%\n",
      "4600: Train accuracy 80.5%, loss: 0.170760\n",
      "Validation accuracy 77.5%\n",
      "4700: Train accuracy 75.0%, loss: 0.223454\n",
      "Validation accuracy 77.6%\n",
      "4800: Train accuracy 77.3%, loss: 0.162185\n",
      "Validation accuracy 77.7%\n",
      "4900: Train accuracy 82.8%, loss: 0.209946\n",
      "Validation accuracy 77.9%\n",
      "5000: Train accuracy 84.4%, loss: 0.196147\n",
      "Validation accuracy 77.9%\n",
      "5100: Train accuracy 79.7%, loss: 0.212640\n",
      "Validation accuracy 78.0%\n",
      "5200: Train accuracy 73.4%, loss: 0.234276\n",
      "Validation accuracy 78.0%\n",
      "5300: Train accuracy 77.3%, loss: 0.163959\n",
      "Validation accuracy 77.8%\n",
      "5400: Train accuracy 83.6%, loss: 0.143559\n",
      "Validation accuracy 78.0%\n",
      "5500: Train accuracy 82.0%, loss: 0.157146\n",
      "Validation accuracy 77.9%\n",
      "5600: Train accuracy 78.1%, loss: 0.161110\n",
      "Validation accuracy 78.1%\n",
      "5700: Train accuracy 78.9%, loss: 0.163855\n",
      "Validation accuracy 78.0%\n",
      "5800: Train accuracy 76.6%, loss: 0.205010\n",
      "Validation accuracy 78.0%\n",
      "5900: Train accuracy 75.0%, loss: 0.181136\n",
      "Validation accuracy 78.1%\n",
      "6000: Train accuracy 79.7%, loss: 0.204680\n",
      "Validation accuracy 78.2%\n",
      "6100: Train accuracy 74.2%, loss: 0.175644\n",
      "Validation accuracy 78.3%\n",
      "6200: Train accuracy 82.0%, loss: 0.155905\n",
      "Validation accuracy 78.4%\n",
      "6300: Train accuracy 80.5%, loss: 0.166137\n",
      "Validation accuracy 78.2%\n",
      "6400: Train accuracy 67.2%, loss: 0.273893\n",
      "Validation accuracy 78.4%\n",
      "6500: Train accuracy 82.0%, loss: 0.140001\n",
      "Validation accuracy 78.5%\n",
      "6600: Train accuracy 79.7%, loss: 0.175618\n",
      "Validation accuracy 78.4%\n",
      "6700: Train accuracy 70.3%, loss: 0.197572\n",
      "Validation accuracy 78.4%\n",
      "6800: Train accuracy 84.4%, loss: 0.127709\n",
      "Validation accuracy 78.4%\n",
      "6900: Train accuracy 87.5%, loss: 0.099764\n",
      "Validation accuracy 78.5%\n",
      "7000: Train accuracy 76.6%, loss: 0.164628\n",
      "Validation accuracy 78.5%\n",
      "7100: Train accuracy 77.3%, loss: 0.128368\n",
      "Validation accuracy 78.3%\n",
      "7200: Train accuracy 69.5%, loss: 0.174976\n",
      "Validation accuracy 78.2%\n",
      "7300: Train accuracy 78.9%, loss: 0.161191\n",
      "Validation accuracy 78.3%\n",
      "7400: Train accuracy 80.5%, loss: 0.151052\n",
      "Validation accuracy 78.2%\n",
      "7500: Train accuracy 74.2%, loss: 0.172488\n",
      "Validation accuracy 78.2%\n",
      "7600: Train accuracy 78.9%, loss: 0.103617\n",
      "Validation accuracy 78.3%\n",
      "7700: Train accuracy 73.4%, loss: 0.163143\n",
      "Validation accuracy 78.4%\n",
      "7800: Train accuracy 81.2%, loss: 0.131856\n",
      "Validation accuracy 78.2%\n",
      "7900: Train accuracy 78.9%, loss: 0.166597\n",
      "Validation accuracy 78.2%\n",
      "Test accuracy 84.6%, loss: 0.135343\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    tf.train.global_step(session, global_step)\n",
    "    print(\"Initialized\")\n",
    "    print(\"Starting autoencoder training\")\n",
    "    for step in range(num_steps_Auto):\n",
    "        start = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[start:(start + batch_size),:]\n",
    "        batch_labels = train_labels[start:(start + batch_size),:]\n",
    "        feed_dict = {tf_train_dataset : batch_data,\n",
    "                    tf_train_labels : batch_labels}\n",
    "        _, lA = session.run([optimizerAuto, lossAuto], \n",
    "                            feed_dict = feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"Step %d, Loss %f\" % (step, lA))\n",
    "    print(\"Finished autoencoder training\")\n",
    "    print(\"Beginning neural net training\")\n",
    "    for step in range(num_steps):\n",
    "        #permute list\n",
    "        #pull things from list in chunks of size batch_size\n",
    "        start = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[start:(start + batch_size),:]\n",
    "        batch_labels = train_labels[start:(start + batch_size),:]\n",
    "        feed_dict = {tf_train_dataset : batch_data,\n",
    "                    tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_pred], \n",
    "                                        feed_dict = feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"%d: Train accuracy %.1f%%, loss: %f\" % (step, accuracy(predictions, batch_labels), l))\n",
    "            print(\"Validation accuracy %.1f%%\" % accuracy(valid_pred.eval(), valid_labels))\n",
    "    print(\"Test accuracy %.1f%%, loss: %f\" % (accuracy(test_pred.eval(), test_labels), l))\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
