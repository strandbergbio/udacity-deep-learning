{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 5\n",
    "------------\n",
    "\n",
    "The goal of this assignment is to train a Word2Vec skip-gram model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0K1ZyLn04QZf"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCjPJE944bkV"
   },
   "source": [
    "Download the data from the source website if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 14640,
     "status": "ok",
     "timestamp": 1445964482948,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "c4ec222c-80b5-4298-e635-93ca9f79c3b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zqz3XiqI4mZT"
   },
   "source": [
    "Read the data into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 28844,
     "status": "ok",
     "timestamp": 1445964497165,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "e3a928b4-1645-4fe8-be17-fcf47de5716d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Build the dictionary and replace rare words with UNK token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 28849,
     "status": "ok",
     "timestamp": 1445964497178,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "3fb4ecd1-df67-44b6-a2dc-2291730970b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1445964901989,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w9APjA-zmfjV",
    "outputId": "67cccb02-cdaf-4e47-d489-43bcc8d57bb8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-447381f90115>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnum_skips\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_window\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ofd1MbBuwiva"
   },
   "source": [
    "Train a skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8pQKsV4Vwlzy"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,\n",
    "                               train_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "  # Compute the similarity between minibatch examples and all embeddings.\n",
    "  # We use the cosine distance:\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "    normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 23
      },
      {
       "item_id": 48
      },
      {
       "item_id": 61
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 436189,
     "status": "ok",
     "timestamp": 1445965429787,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "1bQFGceBxrWW",
    "outputId": "5ebd6d9a-33c6-4bcd-bf6d-252b0b6055e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 8.075641\n",
      "Nearest to s: symbology, fairer, generalization, misskelley, irix, quintet, supercar, berklee,\n",
      "Nearest to during: upsilon, crichton, toon, thinker, drawbacks, lever, antiquities, szl,\n",
      "Nearest to when: floor, mate, comedians, allotropes, cambodia, branson, wow, smallholders,\n",
      "Nearest to time: trouv, applicants, baden, appointment, chunks, haute, asterisk, psoriasis,\n",
      "Nearest to are: ensured, disparate, cali, trying, bookstore, pigeon, brownlow, habitats,\n",
      "Nearest to can: dane, positivism, kneel, plectrum, pack, atlanta, batu, dismiss,\n",
      "Nearest to system: tente, vizier, vintage, welding, aulus, armadillos, belongings, agincourt,\n",
      "Nearest to most: edt, tricyclic, skiers, witnesses, contrasting, folio, ruminants, bolivia,\n",
      "Nearest to UNK: militias, stopover, majin, linebacker, acclamation, consuls, managerial, wpa,\n",
      "Nearest to all: dhul, defending, byu, since, tonality, lleida, vindication, isotropy,\n",
      "Nearest to years: reluctance, genitalia, microbes, using, sham, kellerman, wildcats, sephardic,\n",
      "Nearest to has: timer, spec, shunt, launched, entrepreneurial, scherzo, majuro, marry,\n",
      "Nearest to however: blocked, coppersmith, drags, hit, ere, programmers, existed, bs,\n",
      "Nearest to united: comprises, claimants, hahn, schematic, guns, borough, containers, gaulish,\n",
      "Nearest to as: fur, thirty, possum, surname, ny, gracie, vikernes, insular,\n",
      "Nearest to in: bsa, hdl, kandahar, nucleons, doctoral, lincoln, extremes, alphorn,\n",
      "Average loss at step 2000: 4.361337\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-01669d5c739a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m       batch_size, num_skips, skip_window)\n\u001b[0;32m     10\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0maverage_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += l\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss = average_loss / 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step %d: %f' % (step, average_loss))\n",
    "      average_loss = 0\n",
    "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log = '%s %s,' % (log, close_word)\n",
    "        print(log)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jjJXYA_XzV79"
   },
   "outputs": [],
   "source": [
    "num_points = 400\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "two_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points+1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 4763,
     "status": "ok",
     "timestamp": 1445965465525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2f1ffade4c9f20de",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "o_e0D_UezcDe",
    "outputId": "df22e4a5-e8ec-4e5e-d384-c6cf37c68c34"
   },
   "outputs": [],
   "source": [
    "def plot(embeddings, labels):\n",
    "  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  pylab.figure(figsize=(15,15))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = embeddings[i,:]\n",
    "    pylab.scatter(x, y)\n",
    "    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                   ha='right', va='bottom')\n",
    "  pylab.show()\n",
    "\n",
    "words = [reverse_dictionary[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB5EFrBnpNnc"
   },
   "source": [
    "---\n",
    "\n",
    "Problem\n",
    "-------\n",
    "\n",
    "An alternative to skip-gram is another Word2Vec model called [CBOW](http://arxiv.org/abs/1301.3781) (Continuous Bag of Words). In the CBOW model, instead of predicting a context word from a word vector, you predict a word from the sum of all the word vectors in its context. Implement and evaluate a CBOW model trained on the text8 dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n",
      "\n",
      "with num_skips = 2 and skip_window = 1:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-bd6ff5dd8d73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_CBOW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nwith num_skips = %d and skip_window = %d:'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_skips\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    batch:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    labels:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mli\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mli\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_CBOW(batch_size, skip_window):\n",
    "    global data_index\n",
    "    batch = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 2 * skip_window), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    for i in range(batch_size):\n",
    "        batch[i, 0] = data[data_index % len(data)]\n",
    "        for j in range(skip_window):\n",
    "            pos1 = (data_index - (j + 1)) % len(data)\n",
    "            labels[i,j] = data[pos1]\n",
    "            pos2 = (data_index + (j + 1)) % len(data)\n",
    "            labels[i,j + skip_window] = data[pos2]\n",
    "        data_index += 1    \n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 2\n",
    "    batch, labels = generate_CBOW(batch_size=8, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8 * 2 * skip_window)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Built\n"
     ]
    }
   ],
   "source": [
    "batch_size = 130\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 2 # How many words to consider left and right.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "learning_rate = 1.0\n",
    "decay_steps = 100000\n",
    "decay_rate = 0.8\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 2 * skip_window])\n",
    "    \n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size], stddev = 1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(\n",
    "        tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    context_sum = tf.reduce_sum(tf.nn.embedding_lookup(embeddings, train_labels), 1) / math.sqrt(2.0 * skip_window)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, context_sum, \n",
    "                                   train_dataset, num_sampled, vocabulary_size))\n",
    "    \n",
    "    rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(rate).minimize(loss, global_step=global_step)\n",
    "     \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
    "    \n",
    "    print(\"Graph Built\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step 0: 0.007821\n",
      "['state', 'konoe', 'retake', 'prized', 'backlit', 'antennas', 'saturated', 'atrophy', 'iss']\n",
      "['only', 'testaments', 'slated', 'signers', 'poe', 'engel', 'cardoso', 'wahhabi', 'lodovico']\n",
      "['were', 'demand', 'canton', 'worms', 'stamford', 'extrusive', 'preferring', 'boilers', 'valeria']\n",
      "['system', 'syndicates', 'routes', 'lith', 'onset', 'heisenberg', 'pearse', 'falun', 'manure']\n",
      "['after', 'kidder', 'muonium', 'migrant', 'yielding', 'initiate', 'issued', 'contented', 'ampex']\n",
      "['used', 'chauffeur', 'minucius', 'tuscany', 'skewed', 'gesenius', 'befriends', 'ordinals', 'befall']\n",
      "['over', 'attention', 'hummel', 'calorimeter', 'bedtime', 'turpin', 'abramovich', 'ignoring', 'woe']\n",
      "['been', 'prediction', 'forest', 'bricklin', 'traits', 'ariel', 'mostly', 'volatility', 'oi']\n",
      "['most', 'otter', 'wrapper', 'faraway', 'taif', 'chia', 'vlaanderen', 'zhukov', 'rollo']\n",
      "['from', 'azeotrope', 'dejima', 'judd', 'judgement', 'ixion', 'intercourse', 'fatima', 'debugger']\n",
      "['many', 'brahmagupta', 'identifier', 'lunches', 'assassinations', 'withstand', 'separators', 'paisiello', 'jeannie']\n",
      "['than', 'dealers', 'storm', 'spiritualism', 'uruk', 'chimera', 'duce', 'grandiose', 'achaeans']\n",
      "['th', 'atheromatous', 'cherries', 'visicalc', 'cloak', 'furnishes', 'hag', 'tique', 'brundtland']\n",
      "['known', 'denounce', 'applesoft', 'carcassonne', 'initially', 'klingon', 'freehold', 'covalent', 'randomized']\n",
      "['which', 'informa', 'diabetics', 'dziga', 'levied', 'belief', 'berzelius', 'volcano', 'possibilities']\n",
      "['these', 'corr', 'implicit', 'meditative', 'slumberland', 'liable', 'organized', 'proceeds', 'ztt']\n",
      "Step 1000: 4.860215\n",
      "Step 2000: 4.042772\n",
      "Step 3000: 3.885336\n",
      "Step 4000: 3.744618\n",
      "Step 5000: 3.629110\n",
      "Step 6000: 3.637241\n",
      "Step 7000: 3.585618\n",
      "Step 8000: 3.380748\n",
      "Step 9000: 3.407389\n",
      "Step 10000: 3.484307\n",
      "Step 11000: 3.501892\n",
      "Step 12000: 3.444997\n",
      "Step 13000: 3.447478\n",
      "Step 14000: 3.452807\n",
      "Step 15000: 3.408780\n",
      "Step 16000: 3.458400\n",
      "Step 17000: 3.406478\n",
      "Step 18000: 3.418710\n",
      "Step 19000: 3.261504\n",
      "Step 20000: 3.374670\n",
      "Step 21000: 3.368345\n",
      "Step 22000: 3.342401\n",
      "Step 23000: 3.356747\n",
      "Step 24000: 3.354245\n",
      "Step 25000: 3.301586\n",
      "['state', 'konoe', 'backlit', 'churchland', 'kgb', 'antennas', 'prized', 'beards', 'saturated']\n",
      "['only', 'who', 'career', 'poe', 'magisterial', 'offshoot', 'cisco', 'dew', 'mustaine']\n",
      "['were', 'are', 'was', 'canton', 'exposed', 'is', 'demand', 'transcribing', 'inconsistency']\n",
      "['system', 'syndicates', 'heisenberg', 'pearse', 'olde', 'laminated', 'supplies', 'sense', 'onset']\n",
      "['after', 'kidder', 'muonium', 'migrant', 'succeed', 'jewish', 'intern', 'medulla', 'gemological']\n",
      "['used', 'skewed', 'chauffeur', 'minucius', 'befriends', 'sorcerer', 'humid', 'franc', 'applied']\n",
      "['over', 'during', 'irishman', 'inflict', 'turpin', 'attention', 'commits', 'acheron', 'detract']\n",
      "['been', 'gar', 'served', 'joyful', 'yarns', 'plunder', 'oi', 'refraining', 'antibodies']\n",
      "['most', 'epoxy', 'otter', 'possessive', 'wrapper', 'skaro', 'margaret', 'upstairs', 'macrostate']\n",
      "['from', 'ixion', 'intercourse', 'elsinore', 'rolfe', 'droll', 'through', 'vases', 'spins']\n",
      "['many', 'some', 'these', 'brahmagupta', 'stimulate', 'cans', 'paper', 'topics', 'nucleosome']\n",
      "['than', 'storm', 'or', 'dealers', 'spiritualism', 'duce', 'achaeans', 'tdi', 'cavalli']\n",
      "['th', 'atheromatous', 'furnishes', 'visicalc', 'cloak', 'cherries', 'brundtland', 'widower', 'st']\n",
      "['known', 'denounce', 'carcassonne', 'initially', 'covalent', 'freehold', 'superhuman', 'commercials', 'universe']\n",
      "['which', 'that', 'stunning', 'also', 'informa', 'ballard', 'belief', 'proselytes', 'moderately']\n",
      "['these', 'many', 'some', 'they', 'sean', 'enlai', 'ztt', 'months', 'their']\n",
      "Step 26000: 3.299515\n",
      "Step 27000: 3.347733\n",
      "Step 28000: 3.330144\n",
      "Step 29000: 3.315540\n",
      "Step 30000: 3.297227\n",
      "Step 31000: 3.280056\n",
      "Step 32000: 3.122774\n",
      "Step 33000: 3.164933\n",
      "Step 34000: 3.279701\n",
      "Step 35000: 3.271821\n",
      "Step 36000: 3.235849\n",
      "Step 37000: 3.248262\n",
      "Step 38000: 3.272950\n",
      "Step 39000: 3.199284\n",
      "Step 40000: 3.246093\n",
      "Step 41000: 3.251602\n",
      "Step 42000: 3.265875\n",
      "Step 43000: 3.269991\n",
      "Step 44000: 3.241525\n",
      "Step 45000: 3.222124\n",
      "Step 46000: 3.229174\n",
      "Step 47000: 3.246333\n",
      "Step 48000: 3.161802\n",
      "Step 49000: 3.184734\n",
      "Step 50000: 3.181350\n",
      "['state', 'konoe', 'backlit', 'kgb', 'beards', 'sculpture', 'antennas', 'enveloped', 'churchland']\n",
      "['only', 'magisterial', 'restores', 'career', 'stipulate', 'janis', 'mustaine', 'dew', 'expeditionary']\n",
      "['were', 'are', 'was', 'canton', 'exposed', 'be', 'being', 'transcribing', 'have']\n",
      "['system', 'syndicates', 'pearse', 'olde', 'sense', 'heisenberg', 'casta', 'hieroglyphs', 'laminated']\n",
      "['after', 'before', 'muonium', 'succeed', 'migrant', 'recapture', 'intern', 'when', 'gemological']\n",
      "['used', 'skewed', 'seen', 'applied', 'chauffeur', 'shown', 'befriends', 'franc', 'usefully']\n",
      "['over', 'during', 'attention', 'detract', 'commits', 'inflict', 'jousting', 'irishman', 'syngman']\n",
      "['been', 'become', 'served', 'be', 'gar', 'plunder', 'joyful', 'volatility', 'yarns']\n",
      "['most', 'many', 'some', 'skaro', 'epoxy', 'wrapper', 'southern', 'more', 'margaret']\n",
      "['from', 'ixion', 'through', 'elsinore', 'droll', 'succeed', 'into', 'intercourse', 'rolfe']\n",
      "['many', 'some', 'several', 'these', 'various', 'cans', 'most', 'brahmagupta', 'topics']\n",
      "['than', 'or', 'duce', 'storm', 'spiritualism', 'achaeans', 'cavalli', 'renderings', 'delta']\n",
      "['th', 'atheromatous', 'st', 'cloak', 'furnishes', 'visicalc', 'brundtland', 'widower', 'nine']\n",
      "['known', 'denounce', 'initially', 'commercials', 'carcassonne', 'covalent', 'leaking', 'superhuman', 'dorpat']\n",
      "['which', 'that', 'this', 'what', 'also', 'stunning', 'ballard', 'endings', 'where']\n",
      "['these', 'some', 'many', 'their', 'imitations', 'they', 'several', 'all', 'sean']\n",
      "Step 51000: 3.162434\n",
      "Step 52000: 3.205825\n",
      "Step 53000: 3.180102\n",
      "Step 54000: 3.151776\n",
      "Step 55000: 3.246161\n",
      "Step 56000: 3.143756\n",
      "Step 57000: 2.995528\n",
      "Step 58000: 3.164373\n",
      "Step 59000: 3.146409\n",
      "Step 60000: 3.175223\n",
      "Step 61000: 3.155976\n",
      "Step 62000: 3.132678\n",
      "Step 63000: 3.162427\n",
      "Step 64000: 3.067407\n",
      "Step 65000: 3.029076\n",
      "Step 66000: 3.137790\n",
      "Step 67000: 3.005420\n",
      "Step 68000: 3.055310\n",
      "Step 69000: 3.131006\n",
      "Step 70000: 3.030556\n",
      "Step 71000: 3.121419\n",
      "Step 72000: 3.142046\n",
      "Step 73000: 3.072975\n",
      "Step 74000: 3.020951\n",
      "Step 75000: 3.008563\n",
      "['state', 'backlit', 'konoe', 'sculpture', 'antennas', 'kgb', 'expression', 'sense', 'london']\n",
      "['only', 'stipulate', 'restores', 'morihei', 'magisterial', 'janis', 'expeditionary', 'piro', 'no']\n",
      "['were', 'are', 'was', 'being', 'canton', 'be', 'have', 'had', 'exposed']\n",
      "['system', 'syndicates', 'systems', 'hieroglyphs', 'casta', 'pearse', 'olde', 'sense', 'sanctification']\n",
      "['after', 'before', 'succeed', 'muonium', 'recapture', 'migrant', 'when', 'heraclea', 'tarquinius']\n",
      "['used', 'seen', 'shown', 'applied', 'skewed', 'found', 'licenses', 'chauffeur', 'considered']\n",
      "['over', 'during', 'attention', 'detract', 'syngman', 'jousting', 'inflict', 'commits', 'irishman']\n",
      "['been', 'become', 'be', 'served', 'gar', 'plunder', 'was', 'volatility', 'joyful']\n",
      "['most', 'many', 'some', 'more', 'skaro', 'wrapper', 'aspirated', 'hayden', 'izzard']\n",
      "['from', 'through', 'ixion', 'elsinore', 'droll', 'succeed', 'into', 'lessen', 'after']\n",
      "['many', 'some', 'several', 'these', 'various', 'most', 'all', 'those', 'numerous']\n",
      "['than', 'or', 'achaeans', 'cavalli', 'duce', 'spiritualism', 'storm', 'but', 'renderings']\n",
      "['th', 'atheromatous', 'st', 'nine', 'cloak', 'furnishes', 'visicalc', 'conveys', 'widower']\n",
      "['known', 'denounce', 'commercials', 'regarded', 'leaking', 'initially', 'superhuman', 'covalent', 'carcassonne']\n",
      "['which', 'that', 'this', 'what', 'where', 'also', 'ballard', 'stunning', 'endings']\n",
      "['these', 'many', 'some', 'several', 'all', 'imitations', 'their', 'they', 'cukor']\n",
      "Step 76000: 3.094318\n",
      "Step 77000: 3.118196\n",
      "Step 78000: 3.142464\n",
      "Step 79000: 3.152433\n",
      "Step 80000: 3.116285\n",
      "Step 81000: 3.041690\n",
      "Step 82000: 2.957849\n",
      "Step 83000: 3.096972\n",
      "Step 84000: 3.074029\n",
      "Step 85000: 3.012570\n",
      "Step 86000: 3.079582\n",
      "Step 87000: 3.103161\n",
      "Step 88000: 3.040381\n",
      "Step 89000: 3.125611\n",
      "Step 90000: 3.081929\n",
      "Step 91000: 2.955960\n",
      "Step 92000: 3.024299\n",
      "Step 93000: 3.070586\n",
      "Step 94000: 3.041078\n",
      "Step 95000: 3.046744\n",
      "Step 96000: 3.060857\n",
      "Step 97000: 3.006818\n",
      "Step 98000: 2.854248\n",
      "Step 99000: 2.431780\n",
      "Step 100000: 2.822210\n",
      "['state', 'backlit', 'sculpture', 'london', 'expression', 'antennas', 'konoe', 'sense', 'kgb']\n",
      "['only', 'restores', 'no', 'stipulate', 'morihei', 'janis', 'magisterial', 'generally', 'mustaine']\n",
      "['were', 'are', 'was', 'being', 'had', 'have', 'be', 'canton', 'transcribing']\n",
      "['system', 'systems', 'syndicates', 'hieroglyphs', 'casta', 'pearse', 'olde', 'signifies', 'sanctification']\n",
      "['after', 'before', 'during', 'muonium', 'recapture', 'updates', 'when', 'succeed', 'grayson']\n",
      "['used', 'seen', 'applied', 'shown', 'written', 'found', 'knock', 'considered', 'referred']\n",
      "['over', 'during', 'detract', 'attention', 'syngman', 'jousting', 'politely', 'within', 'alkoxide']\n",
      "['been', 'become', 'be', 'served', 'plunder', 'gar', 'was', 'come', 'volatility']\n",
      "['most', 'many', 'more', 'some', 'aspirated', 'skaro', 'izzard', 'hayden', 'knuth']\n",
      "['from', 'through', 'ixion', 'elsinore', 'lessen', 'droll', 'after', 'near', 'succeed']\n",
      "['many', 'some', 'several', 'various', 'all', 'most', 'these', 'numerous', 'both']\n",
      "['than', 'or', 'achaeans', 'duce', 'cavalli', 'storm', 'but', 'spiritualism', 'renderings']\n",
      "['th', 'st', 'atheromatous', 'furnishes', 'conveys', 'nineteenth', 'cloak', 'brundtland', 'visicalc']\n",
      "['known', 'denounce', 'regarded', 'commercials', 'leaking', 'initially', 'available', 'superhuman', 'referred']\n",
      "['which', 'that', 'this', 'what', 'also', 'where', 'ballard', 'these', 'endings']\n",
      "['these', 'many', 'some', 'several', 'imitations', 'different', 'their', 'all', 'various']\n",
      "Step 101000: 2.731145\n",
      "Step 102000: 2.912111\n",
      "Step 103000: 3.001060\n",
      "Step 104000: 3.006017\n",
      "Step 105000: 2.955863\n",
      "Step 106000: 3.012203\n",
      "Step 107000: 3.091496\n",
      "Step 108000: 3.051577\n",
      "Step 109000: 3.034565\n",
      "Step 110000: 3.019661\n",
      "Step 111000: 2.789554\n",
      "Step 112000: 2.988121\n",
      "Step 113000: 3.044034\n",
      "Step 114000: 3.010721\n",
      "Step 115000: 3.026452\n",
      "Step 116000: 3.063466\n",
      "Step 117000: 3.013434\n",
      "Step 118000: 3.075785\n",
      "Step 119000: 2.785475\n",
      "Step 120000: 2.780667\n",
      "Step 121000: 2.874363\n",
      "Step 122000: 2.979852\n",
      "Step 123000: 2.790321\n",
      "Step 124000: 2.944829\n",
      "Step 125000: 3.082145\n",
      "['state', 'sculpture', 'backlit', 'beards', 'london', 'expression', 'federal', 'antennas', 'kgb']\n",
      "['only', 'restores', 'no', 'uncommon', 'stipulate', 'janis', 'morihei', 'generally', 'rotterdam']\n",
      "['were', 'are', 'was', 'being', 'had', 'have', 'be', 'canton', 'transcribing']\n",
      "['system', 'systems', 'syndicates', 'signifies', 'olde', 'casta', 'hieroglyphs', 'pearse', 'interface']\n",
      "['after', 'before', 'during', 'recapture', 'afterwards', 'grayson', 'intern', 'when', 'muonium']\n",
      "['used', 'seen', 'shown', 'applied', 'written', 'found', 'referred', 'known', 'licenses']\n",
      "['over', 'during', 'attention', 'detract', 'syngman', 'catalunya', 'within', 'politely', 'alkoxide']\n",
      "['been', 'become', 'be', 'served', 'was', 'were', 'plunder', 'come', 'gar']\n",
      "['most', 'many', 'some', 'more', 'aspirated', 'izzard', 'hayden', 'nf', 'amalric']\n",
      "['from', 'through', 'ixion', 'droll', 'lessen', 'succeed', 'debugger', 'after', 'into']\n",
      "['many', 'some', 'several', 'various', 'most', 'both', 'numerous', 'all', 'these']\n",
      "['than', 'or', 'achaeans', 'duce', 'cavalli', 'but', 'spiritualism', 'storm', 'renderings']\n",
      "['th', 'st', 'atheromatous', 'nineteenth', 'conveys', 'furnishes', 'nd', 'cloak', 'brundtland']\n",
      "['known', 'regarded', 'denounce', 'leaking', 'commercials', 'see', 'available', 'referred', 'superhuman']\n",
      "['which', 'that', 'this', 'what', 'where', 'also', 'ballard', 'these', 'endings']\n",
      "['these', 'many', 'different', 'imitations', 'various', 'some', 'several', 'which', 'their']\n",
      "Step 126000: 2.972578\n",
      "Step 127000: 2.885433\n",
      "Step 128000: 2.990716\n",
      "Step 129000: 3.009886\n",
      "Step 130000: 3.038177\n",
      "Step 131000: 3.031204\n",
      "Step 132000: 2.922985\n",
      "Step 133000: 2.876892\n",
      "Step 134000: 2.943131\n",
      "Step 135000: 2.882156\n",
      "Step 136000: 2.841991\n",
      "Step 137000: 2.887018\n",
      "Step 138000: 2.916996\n",
      "Step 139000: 2.753770\n",
      "Step 140000: 2.799671\n",
      "Step 141000: 2.908749\n",
      "Step 142000: 2.930730\n",
      "Step 143000: 2.867558\n",
      "Step 144000: 2.867094\n",
      "Step 145000: 2.960415\n",
      "Step 146000: 2.907647\n",
      "Step 147000: 2.930717\n",
      "Step 148000: 2.903930\n",
      "Step 149000: 2.910791\n",
      "Step 150000: 2.647622\n",
      "['state', 'sculpture', 'backlit', 'federal', 'beards', 'expression', 'antennas', 'government', 'kgb']\n",
      "['only', 'uncommon', 'restores', 'no', 'magisterial', 'generally', 'janis', 'morihei', 'stipulate']\n",
      "['were', 'are', 'was', 'being', 'had', 'have', 'be', 'canton', 'been']\n",
      "['system', 'systems', 'syndicates', 'hieroglyphs', 'olde', 'signifies', 'interface', 'sanctification', 'pearse']\n",
      "['after', 'before', 'during', 'afterwards', 'recapture', 'baines', 'when', 'intern', 'grayson']\n",
      "['used', 'seen', 'applied', 'shown', 'written', 'referred', 'found', 'known', 'proportions']\n",
      "['over', 'during', 'attention', 'detract', 'syngman', 'catalunya', 'within', 'politely', 'alkoxide']\n",
      "['been', 'become', 'be', 'served', 'was', 'come', 'were', 'plunder', 'stability']\n",
      "['most', 'many', 'more', 'some', 'aspirated', 'izzard', 'hayden', 'southern', 'nf']\n",
      "['from', 'through', 'droll', 'ixion', 'lessen', 'succeed', 'elsinore', 'into', 'longest']\n",
      "['many', 'some', 'several', 'various', 'numerous', 'most', 'both', 'all', 'these']\n",
      "['than', 'or', 'achaeans', 'duce', 'cavalli', 'renderings', 'but', 'spiritualism', 'storm']\n",
      "['th', 'st', 'atheromatous', 'nineteenth', 'nd', 'conveys', 'furnishes', 'twentieth', 'persuasion']\n",
      "['known', 'regarded', 'denounce', 'commercials', 'leaking', 'available', 'referred', 'see', 'described']\n",
      "['which', 'that', 'this', 'what', 'where', 'also', 'ballard', 'these', 'endings']\n",
      "['these', 'various', 'different', 'imitations', 'many', 'those', 'several', 'some', 'their']\n",
      "Step 151000: 2.946428\n",
      "Step 152000: 2.899623\n",
      "Step 153000: 2.926555\n",
      "Step 154000: 2.919929\n",
      "Step 155000: 2.910868\n",
      "Step 156000: 2.949155\n",
      "Step 157000: 2.862111\n",
      "Step 158000: 2.918550\n",
      "Step 159000: 2.923867\n",
      "Step 160000: 2.922708\n",
      "Step 161000: 2.849856\n",
      "Step 162000: 2.852054\n",
      "Step 163000: 2.637405\n",
      "Step 164000: 2.833001\n",
      "Step 165000: 2.892313\n",
      "Step 166000: 2.885163\n",
      "Step 167000: 2.853333\n",
      "Step 168000: 2.887174\n",
      "Step 169000: 2.874305\n",
      "Step 170000: 2.845093\n",
      "Step 171000: 2.889522\n",
      "Step 172000: 2.897585\n",
      "Step 173000: 2.942356\n",
      "Step 174000: 2.909655\n",
      "Step 175000: 2.912050\n",
      "['state', 'backlit', 'sculpture', 'federal', 'expression', 'government', 'beards', 'sense', 'speleological']\n",
      "['only', 'uncommon', 'restores', 'magisterial', 'generally', 'no', 'always', 'janis', 'rotterdam']\n",
      "['were', 'are', 'was', 'being', 'had', 'have', 'be', 'canton', 'been']\n",
      "['system', 'systems', 'syndicates', 'hieroglyphs', 'interface', 'olde', 'signifies', 'sanctification', 'method']\n",
      "['after', 'before', 'during', 'afterwards', 'recapture', 'grayson', 'when', 'baines', 'following']\n",
      "['used', 'applied', 'seen', 'shown', 'found', 'written', 'referred', 'known', 'designed']\n",
      "['over', 'during', 'attention', 'detract', 'syngman', 'politely', 'catalunya', 'away', 'within']\n",
      "['been', 'become', 'be', 'served', 'come', 'was', 'plunder', 'were', 'volatility']\n",
      "['most', 'some', 'more', 'many', 'less', 'highly', 'aspirated', 'hayden', 'izzard']\n",
      "['from', 'through', 'ixion', 'droll', 'elsinore', 'into', 'rda', 'worries', 'alomari']\n",
      "['many', 'some', 'several', 'various', 'numerous', 'both', 'most', 'all', 'few']\n",
      "['than', 'or', 'duce', 'achaeans', 'but', 'renderings', 'storm', 'cavalli', 'spiritualism']\n",
      "['th', 'st', 'atheromatous', 'nineteenth', 'nd', 'conveys', 'twentieth', 'nine', 'brundtland']\n",
      "['known', 'regarded', 'referred', 'available', 'leaking', 'denounce', 'commercials', 'see', 'viewed']\n",
      "['which', 'that', 'this', 'what', 'where', 'also', 'ballard', 'endings', 'these']\n",
      "['these', 'various', 'different', 'imitations', 'those', 'which', 'several', 'many', 'some']\n",
      "Step 176000: 2.887101\n",
      "Step 177000: 2.913553\n",
      "Step 178000: 2.925268\n",
      "Step 179000: 2.790967\n",
      "Step 180000: 2.886194\n",
      "Step 181000: 2.798251\n",
      "Step 182000: 2.890772\n",
      "Step 183000: 2.899802\n",
      "Step 184000: 2.885517\n",
      "Step 185000: 2.853645\n",
      "Step 186000: 2.941849\n",
      "Step 187000: 2.841425\n",
      "Step 188000: 2.682497\n",
      "Step 189000: 2.859589\n",
      "Step 190000: 2.832883\n",
      "Step 191000: 2.896181\n",
      "Step 192000: 2.886316\n",
      "Step 193000: 2.847954\n",
      "Step 194000: 2.844974\n",
      "Step 195000: 2.816834\n",
      "Step 196000: 2.734492\n",
      "Step 197000: 2.862826\n",
      "Step 198000: 2.726620\n",
      "Step 199000: 2.783567\n",
      "Step 200000: 2.870590\n",
      "['state', 'sculpture', 'backlit', 'expression', 'federal', 'antennas', 'government', 'speleological', 'sense']\n",
      "['only', 'uncommon', 'restores', 'no', 'magisterial', 'generally', 'always', 'morihei', 'stipulate']\n",
      "['were', 'are', 'was', 'being', 'had', 'have', 'be', 'became', 'canton']\n",
      "['system', 'systems', 'syndicates', 'hieroglyphs', 'interface', 'signifies', 'olde', 'method', 'sanctification']\n",
      "['after', 'before', 'during', 'afterwards', 'when', 'following', 'recapture', 'grayson', 'baines']\n",
      "['used', 'seen', 'applied', 'shown', 'referred', 'written', 'found', 'designed', 'known']\n",
      "['over', 'during', 'attention', 'detract', 'syngman', 'across', 'catalunya', 'away', 'within']\n",
      "['been', 'become', 'be', 'served', 'was', 'come', 'plunder', 'were', 'volatility']\n",
      "['most', 'some', 'many', 'more', 'less', 'highly', 'aspirated', 'hayden', 'izzard']\n",
      "['from', 'through', 'ixion', 'droll', 'rda', 'debugger', 'into', 'lessen', 'interstates']\n",
      "['many', 'some', 'several', 'various', 'numerous', 'all', 'most', 'both', 'few']\n",
      "['than', 'or', 'but', 'achaeans', 'duce', 'renderings', 'cavalli', 'storm', 'nonverbal']\n",
      "['th', 'st', 'atheromatous', 'nd', 'nineteenth', 'conveys', 'twentieth', 'furnishes', 'brundtland']\n",
      "['known', 'referred', 'regarded', 'available', 'leaking', 'denounce', 'commercials', 'viewed', 'see']\n",
      "['which', 'that', 'this', 'what', 'where', 'also', 'these', 'endings', 'ballard']\n",
      "['these', 'various', 'different', 'imitations', 'those', 'several', 'which', 'many', 'certain']\n",
      "Step 201000: 2.733569\n",
      "Step 202000: 2.912685\n",
      "Step 203000: 2.870205\n",
      "Step 204000: 2.799504\n",
      "Step 205000: 2.662067\n",
      "Step 206000: 2.834670\n",
      "Step 207000: 2.864431\n",
      "Step 208000: 2.879614\n",
      "Step 209000: 2.882899\n",
      "Step 210000: 2.901627\n",
      "Step 211000: 2.853715\n",
      "Step 212000: 2.764022\n",
      "Step 213000: 2.737392\n",
      "Step 214000: 2.890970\n",
      "Step 215000: 2.836068\n",
      "Step 216000: 2.775653\n",
      "Step 217000: 2.856583\n",
      "Step 218000: 2.864106\n",
      "Step 219000: 2.805489\n",
      "Step 220000: 2.908571\n",
      "Step 221000: 2.808835\n",
      "Step 222000: 2.697087\n",
      "Step 223000: 2.863659\n",
      "Step 224000: 2.856516\n",
      "Step 225000: 2.787759\n",
      "['state', 'sculpture', 'backlit', 'expression', 'government', 'federal', 'beards', 'marcantonio', 'speleological']\n",
      "['only', 'uncommon', 'restores', 'no', 'even', 'always', 'generally', 'stipulate', 'weld']\n",
      "['were', 'are', 'was', 'being', 'had', 'have', 'be', 'been', 'transcribing']\n",
      "['system', 'systems', 'syndicates', 'ministerial', 'hieroglyphs', 'routes', 'vijayanagara', 'method', 'olde']\n",
      "['after', 'before', 'during', 'afterwards', 'following', 'when', 'recapture', 'grayson', 'until']\n",
      "['used', 'applied', 'seen', 'referred', 'shown', 'designed', 'written', 'known', 'found']\n",
      "['over', 'during', 'attention', 'detract', 'syngman', 'across', 'within', 'marginally', 'under']\n",
      "['been', 'become', 'be', 'was', 'served', 'come', 'plunder', 'were', 'volatility']\n",
      "['most', 'many', 'some', 'more', 'less', 'highly', 'hayden', 'aspirated', 'izzard']\n",
      "['from', 'through', 'ixion', 'droll', 'interstates', 'into', 'rda', 'debugger', 'elsinore']\n",
      "['many', 'some', 'several', 'various', 'numerous', 'most', 'all', 'both', 'few']\n",
      "['than', 'or', 'duce', 'but', 'achaeans', 'cavalli', 'renderings', 'khyber', 'storm']\n",
      "['th', 'st', 'nineteenth', 'nd', 'atheromatous', 'conveys', 'twentieth', 'brundtland', 'furnishes']\n",
      "['known', 'referred', 'regarded', 'available', 'leaking', 'denounce', 'commercials', 'viewed', 'described']\n",
      "['which', 'that', 'this', 'where', 'what', 'also', 'these', 'ballard', 'endings']\n",
      "['these', 'various', 'different', 'those', 'imitations', 'which', 'several', 'certain', 'they']\n",
      "Step 226000: 2.833766\n",
      "Step 227000: 2.824006\n",
      "Step 228000: 2.812326\n",
      "Step 229000: 2.493123\n",
      "Step 230000: 2.321424\n",
      "Step 231000: 2.575110\n",
      "Step 232000: 2.543789\n",
      "Step 233000: 2.737313\n",
      "Step 234000: 2.797865\n",
      "Step 235000: 2.813285\n",
      "Step 236000: 2.741745\n",
      "Step 237000: 2.827044\n",
      "Step 238000: 2.861136\n",
      "Step 239000: 2.834553\n",
      "Step 240000: 2.843964\n",
      "Step 241000: 2.750544\n",
      "Step 242000: 2.616782\n",
      "Step 243000: 2.815488\n",
      "Step 244000: 2.817080\n",
      "Step 245000: 2.834123\n",
      "Step 246000: 2.783152\n",
      "Step 247000: 2.899519\n",
      "Step 248000: 2.805330\n",
      "Step 249000: 2.848046\n",
      "Step 250000: 2.565067\n",
      "['state', 'sculpture', 'backlit', 'government', 'beards', 'federal', 'expression', 'speleological', 'marcantonio']\n",
      "['only', 'uncommon', 'restores', 'always', 'no', 'generally', 'stipulate', 'even', 'particularly']\n",
      "['were', 'are', 'was', 'being', 'had', 'have', 'been', 'be', 'transcribing']\n",
      "['system', 'systems', 'syndicates', 'ministerial', 'signifies', 'vijayanagara', 'routes', 'hieroglyphs', 'method']\n",
      "['after', 'before', 'during', 'afterwards', 'following', 'when', 'grayson', 'until', 'without']\n",
      "['used', 'applied', 'seen', 'referred', 'shown', 'designed', 'written', 'known', 'use']\n",
      "['over', 'during', 'attention', 'across', 'detract', 'away', 'syngman', 'catalunya', 'marginally']\n",
      "['been', 'become', 'be', 'was', 'come', 'served', 'were', 'plunder', 'volatility']\n",
      "['most', 'some', 'many', 'more', 'less', 'highly', 'aspirated', 'hayden', 'southern']\n",
      "['from', 'through', 'ixion', 'droll', 'debugger', 'rda', 'lessen', 'interstates', 'biz']\n",
      "['many', 'some', 'several', 'various', 'numerous', 'both', 'most', 'all', 'few']\n",
      "['than', 'or', 'but', 'duce', 'achaeans', 'cavalli', 'condominium', 'khyber', 'storm']\n",
      "['th', 'st', 'nineteenth', 'nd', 'atheromatous', 'conveys', 'twentieth', 'rd', 'furnishes']\n",
      "['known', 'referred', 'regarded', 'available', 'leaking', 'commercials', 'denounce', 'viewed', 'see']\n",
      "['which', 'that', 'this', 'where', 'what', 'also', 'ballard', 'these', 'endings']\n",
      "['these', 'various', 'different', 'those', 'imitations', 'certain', 'which', 'several', 'many']\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_steps = 250001\n",
    "data_index = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_CBOW(batch_size, skip_window)\n",
    "        feed_dict = {train_dataset : batch_data,\n",
    "                    train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict = feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 1000 == 0:\n",
    "            average_loss = average_loss / 1000\n",
    "            print(\"Step %d: %f\" % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        if step % 25000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            top_k = 8\n",
    "            for i in range(valid_size):\n",
    "                w = []\n",
    "                w.append(reverse_dictionary[valid_examples[i]])\n",
    "                nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                for k in range(top_k):\n",
    "                    w.append(reverse_dictionary[nearest[k]])\n",
    "                print(w)     \n",
    "                del(w)\n",
    "    print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "5_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
